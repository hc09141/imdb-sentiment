{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\Anaconda3\\envs\\421-c\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import string\n",
    "from scipy import spatial\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets positive and negative review folders\n",
    "base_data_dir = \"..\" + os.sep + \"txt_sentoken\"\n",
    "pos_files = base_data_dir + os.sep + 'pos'\n",
    "neg_files = base_data_dir + os.sep + 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(directory, label):\n",
    "    reviews = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file = directory + os.sep + filename\n",
    "        with smart_open.smart_open(file, encoding=\"iso-8859-1\") as f:\n",
    "            review = bytearray()\n",
    "            for i, line in enumerate(f):\n",
    "                review += line\n",
    "            reviews.append({\"review\": review, \"label\": label})\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converts files\n",
    "pos_reviews = get_files(pos_files, 1)\n",
    "neg_reviews = get_files(neg_files, 0)\n",
    "p = list(pos_reviews)\n",
    "n = list(neg_reviews)\n",
    "random.shuffle(p)\n",
    "random.shuffle(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(filename, tokens_only=False):\n",
    "    with smart_open.smart_open(filename, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train_split(docs):\n",
    "    test = []\n",
    "    train = []\n",
    "    labels = {}\n",
    "    count = 2\n",
    "    for i, doc_set in enumerate(docs):\n",
    "        for j, doc in enumerate(doc_set):\n",
    "            if j < len(doc_set) / 2:\n",
    "                test.append({\"doc\": gensim.utils.simple_preprocess(doc_set[j][\"review\"]), \"label\": doc_set[j][\"label\"]})\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                res = gensim.models.doc2vec.TaggedDocument(\n",
    "                    gensim.utils.simple_preprocess(doc_set[j][\"review\"]), [count, doc_set[j][\"label\"]])\n",
    "                if doc_set[j][\"label\"] in labels:\n",
    "                    labels[doc_set[j][\"label\"]].append(count)\n",
    "                else:\n",
    "                    labels[doc_set[j][\"label\"]] = [count]\n",
    "                count += 1\n",
    "                train.append(res)\n",
    "    return test, train, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_corpus, train_corpus, train_labels = test_train_split([p, n])\n",
    "random.shuffle(train_corpus)\n",
    "# print(train_corpus)\n",
    "# print(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(size=100, min_count=5, iter=100, dm=0, dbow_words=1, window=5) \n",
    "# size is vector size\n",
    "# min_count is the number of times a word needs to be used\n",
    "#iter is number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets vocab\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45848237"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trains model on texts\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('score', 0.5738385319709778),\n",
       " ('music', 0.5452611446380615),\n",
       " ('singers', 0.4798170030117035),\n",
       " ('band', 0.4696308672428131),\n",
       " ('broadway', 0.4524494409561157),\n",
       " ('numbers', 0.4466969668865204),\n",
       " ('soundtrack', 0.44653695821762085),\n",
       " ('number', 0.4410157799720764),\n",
       " ('sequences', 0.44027411937713623),\n",
       " ('meatloaf', 0.42949533462524414)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"musical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macdowell', 0.44383466243743896),\n",
       " ('clarence', 0.4428166151046753),\n",
       " ('jenna', 0.4401274025440216),\n",
       " ('housewife', 0.43081384897232056),\n",
       " ('connick', 0.4238712191581726),\n",
       " ('feisty', 0.42261895537376404),\n",
       " ('natured', 0.41716793179512024),\n",
       " ('maurice', 0.4149838089942932),\n",
       " ('contribute', 0.4112398624420166),\n",
       " ('catchy', 0.41089701652526855)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"jazz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('role', 0.5017963647842407),\n",
       " ('baranski', 0.4889695644378662),\n",
       " ('forlani', 0.48303040862083435),\n",
       " ('actor', 0.4811933636665344),\n",
       " ('performance', 0.47210466861724854),\n",
       " ('marisa', 0.47180262207984924),\n",
       " ('preston', 0.4624374210834503),\n",
       " ('natasha', 0.4604119062423706),\n",
       " ('played', 0.45575565099716187),\n",
       " ('nina', 0.4555533230304718)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"actress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assess model\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# counts what ranks each document was classified as\n",
    "# collections.Counter(ranks)  # Results vary due to random seeding and very small corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "k = 5\n",
    "for doc in test_corpus:\n",
    "    inferred_vector = model.infer_vector(doc[\"doc\"])\n",
    "    k_correct = 0\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    j = 0\n",
    "    for i in range(0, k):\n",
    "        while sims[j][0] < 2:\n",
    "            j += 1\n",
    "        if (sims[j][0]) in train_labels[doc[\"label\"]]:\n",
    "            k_correct += 1\n",
    "    if k_correct > k / 2:\n",
    "        correct += 1\n",
    "print(correct / len(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./imdb-k-nearest.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "421-c",
   "language": "python",
   "name": "421-c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
